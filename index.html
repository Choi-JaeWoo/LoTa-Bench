<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LoTa-Bench</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

  
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://choi-jaewoo.github.io/">Jae-Woo Choi</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/youngwoo-yoon/">Youngwoo Yoon</a><sup>1*</sup>,</span>
            <span class="author-block">
              Hyobin Ong<sup>1, 2</sup>,
            </span>
            <span class="author-block">
              Jaehong Kim<sup>1</sup>,
            </span>
            <span class="author-block">
              Minsu Jang<sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Electronics and Telecommunications Research Institute, <sup>2</sup>University of Science and Technology</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=ADSxCpCu9s"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.08178"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lbaa2022/LLMTaskPlanning"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code & Data</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <figure>
          <img src="./static/images/overview.png" alt="LoTa-Bench Overview Image" style="max-width:100%;height:auto;">
          <figcaption>LoTa-Bench Overview</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Benchmark Suites. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Benchmark Suites</h2>
        <div class="content has-text-justified">
          <p>
            To rigorously evaluate LLM-based task planners, we introduce a comprehensive evaluation framework, LoTa-Bench. The framework integrates three key components: a task planner, a dataset, and a simulator. Our baseline task planner leverages the in-context learning capabilities of LLMs. Then we offer two distinct dataset-simulator pairings: 1) the ALFRED dataset built on the AI2-THOR simulator, and 2) an extended version of the Watch-And-Help (WAH) dataset, named WAH-NL, incorporated with the VirtualHome simulator. The following images depict samples where the LLM-based task planner, utilizing the GPT-3 175B model, has successfully planned and executed desired tasks within the simulator.
          </p>
        </div>
        <figure class="has-text-centered">
          <img src="./static/images/alfred_success.png" alt="Benchmark Suite 1 Image" style="max-width:100%;height:auto;">
          <figcaption>ALFRED and AI2-THOR: success example</figcaption>
        </figure>
        <figure class="has-text-centered">
          <img src="./static/images/vh_success.png" alt="Benchmark Suite 2 Image" style="max-width:100%;height:auto;">
          <figcaption>WAH-NL and VirtualHome: success example</figcaption>
        </figure>
      </div>
    </div>

    <!-- Base Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Base Experiments</h2>
        <div class="content has-text-justified">
          <p>
            We conducted experiments to measure the performance of the baseline LLM-based task planners by using the proposed benchmark. We tested various settings including LLMs in different model classes and sizes and the impact of the number of in-context examples. The following image shows the results on ALFRED and WAH-NL for different pre-trained LLMs.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-half">
            <figure class="has-text-centered">
              <img src="./static/images/alfred_llm_model.png" alt="Base Experiment 1 Image" style="max-width:100%;height:auto;">
              <figcaption>Baseline results on ALFRED</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="has-text-centered">
              <img src="./static/images/wah_llm_model.png" alt="Based Experiment 2 Image" style="max-width:100%;height:auto;">
              <figcaption>Baseline results on WAH-NL</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- Extensions of Task Planner. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Extensions of Task Planner</h2>
        <div class="content has-text-justified">
          <p>
            The primary merit of the proposed benchmark is that it allows faster and easier validation of new task planners. To demonstrate this, we explore some extensions (or improvements) of the base planner and validate them.
          </p>
        </div>

        <!-- Two columns layout -->
        <div class="columns">
          <!-- In-Context Example Selection. -->
          <div class="column">
            <h3 class="title is-4">In-Context Example Selection</h3>
            <div class="content">
              <p>
                We explored three strategies for selecting in-context examples from the train set: Random Sampling, Task-Specific Sampling, and Semantic Similarity Sampling. We found that across all model sizes, Semantic Similarity Sampling showed superior performance, followed by Task-Specific Sampling, and lastly Random Sampling.
              </p>
            </div>
            <figure class="has-text-centered">
              <img src="./static/images/ic_select.png" alt="Extension Experiment 1 Image" style="max-width:100%;height:auto;">
              <figcaption>Subgoal success rate for different in-context example selection strategies on WAH-NL</figcaption>
            </figure>
          </div>
      
          <!-- Fine-tuning on Train Set. -->
          <div class="column">
            <h3 class="title is-4">Fine-tuning on Train Set</h3>
            <div class="content">
              <p>
                We conducted experiments to investigate the potential to improve planner performance through fine-tuning. We fine-tuned LLaMA 1 models using LoRA on the ALFRED train set and evaluated their performance in the same ALFRED domain. As depicted in the following figure, fine-tuning significantly improved performance.
              </p>
            </div>
            <figure class="has-text-centered">
              <img src="./static/images/finetuning.png" alt="Experiment 2 Image" style="max-width:100%;height:auto;">
              <figcaption>Success rate of fine-tuned planners on ALFRED</figcaption>
            </figure>
          </div>
        </div>
        
        <style>
          .image-container {
            display: flex; /* Flexbox를 사용해 이미지를 정렬합니다 */
            align-items: center; /* 이미지를 세로 중앙에 배치합니다 */
            justify-content: center; /* 이미지를 가로 중앙에 배치합니다 */
            height: 200px; /* 또는 원하는 높이로 설정합니다 */
            overflow: hidden; /* 컨테이너를 넘어서는 부분은 숨깁니다 */
          }
          .image-container img {
            height: 100%; /* 이미지의 높이를 컨테이너의 높이에 맞춥니다 */
            width: auto; /* 이미지의 원래 비율을 유지합니다 */
          }
        </style>
        
        <div class="columns">
          <!-- In-Context Example Selection. -->
          <div class="column">
            <h3 class="title is-4">In-Context Example Selection</h3>
            <div class="content">
              <p>
                We explored three strategies for selecting in-context examples from the train set: Random Sampling, Task-Specific Sampling, and Semantic Similarity Sampling. We found that across all model sizes, Semantic Similarity Sampling showed superior performance, followed by Task-Specific Sampling, and lastly Random Sampling.
              </p>
            </div>
            <figure class="has-text-centered image-container">
              <img src="./static/images/ic_select.png" alt="Extension Experiment 1 Image">
              <figcaption>Subgoal success rate for different in-context example selection strategies on WAH-NL</figcaption>
            </figure>
          </div>
        
          <!-- Fine-tuning on Train Set. -->
          <div class="column">
            <h3 class="title is-4">Fine-tuning on Train Set</h3>
            <div class="content">
              <p>
                We conducted experiments to investigate the potential to improve planner performance through fine-tuning. We fine-tuned LLaMA 1 models using LoRA on the ALFRED train set and evaluated their performance in the same ALFRED domain. As depicted in the following figure, fine-tuning significantly improved performance.
              </p>
            </div>
            <figure class="has-text-centered image-container">
              <img src="./static/images/finetuning.png" alt="Experiment 2 Image">
              <figcaption>Success rate of fine-tuned planners on ALFRED</figcaption>
            </figure>
          </div>
        </div>

        <!-- Feedback and Replanning. -->
        <div class="column">
          <h3 class="title is-4">Feedback and Replanning</h3>
          <div class="content">
            <p>
              Adjusting the plan in response to the failure of the action is necessary for task planning in the wild. We investigated whether our LLM-based task planner can reflect feedback from action failure and replan appropriately. The qualitative results showing how the planner succeeded by replanning steps are shown in the following figure.
            </p>
          </div>
          <figure class="has-text-centered">
            <img src="./static/images/replanning.png" alt="Extension Experiment 3 Image" style="max-width:100%;height:auto;">
            <figcaption>Success case of replanning on ALFRED</figcaption>
          </figure>
        </div>


      </div>
    </div>
  </div>

    

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{choi2024lota,
  title={LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents},
  author={Choi, Jae-Woo and Yoon, Youngwoo and Ong, Hyobin and Kim, Jaehong and Jang, Minsu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/pdf/LoTa_Bench__ICLR_24_Camera_Ready.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/lbaa2022/LLMTaskPlanning" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
